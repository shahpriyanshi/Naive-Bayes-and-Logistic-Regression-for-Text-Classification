{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84dDU9OV1qkT",
        "outputId": "92d27925-2bbd-43ff-83f2-718e0db5a7b8"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# import statments\n",
        "import numpy\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def tokenize(sentences):\n",
        "  words = []\n",
        "  for sentence in sentences:\n",
        "    w = word_extraction(sentence)\n",
        "    words.extend(w)\n",
        "  # words = sorted(list(set(words)))\n",
        "  return words\n",
        "\n",
        "def word_extraction(sentence):\n",
        "  ignore = ['a', \"the\", \"is\"]\n",
        "  words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
        "  cleaned_text = [w.lower() for w in words if len(w)>2 and w not in ignore]\n",
        "  return cleaned_text  \n",
        "\n",
        "def generate_bow(allsentences):\n",
        "  S=[]\n",
        "  vocab = tokenize(allsentences)\n",
        "  vocab = sorted(list(set(vocab)))\n",
        "  print(len(vocab))\n",
        "\n",
        "  for sentence in allsentences:\n",
        "    words = word_extraction(sentence)\n",
        "    bag_vector = numpy.zeros(len(vocab))\n",
        "\n",
        "    for w in words:\n",
        "        for i,word in enumerate(vocab):\n",
        "            if word == w: \n",
        "                bag_vector[i] += 1\n",
        "                      \n",
        "    S.append(numpy.array(bag_vector))\n",
        "    numpy.set_printoptions(threshold=sys.maxsize)\n",
        "  return(S)\n",
        "\n",
        "def generate_bernoulli(allsentences):\n",
        "  S=[]\n",
        "  vocab = tokenize(allsentences)\n",
        "  vocab = sorted(list(set(vocab)))\n",
        "  print(len(vocab))\n",
        "\n",
        "  for sentence in allsentences:\n",
        "    words = word_extraction(sentence)\n",
        "    bag_vector = numpy.zeros(len(vocab))\n",
        "\n",
        "    for w in words:\n",
        "        for i,word in enumerate(vocab):\n",
        "            if word == w: \n",
        "                bag_vector[i] = 1\n",
        "                      \n",
        "    S.append(numpy.array(bag_vector))\n",
        "    numpy.set_printoptions(threshold=sys.maxsize)\n",
        "  return(S)\n",
        "\n",
        "#dictionary\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "dictionary=[]\n",
        "classes=['spam', 'ham']\n",
        "lbl=[]\n",
        "\n",
        "path=\"Dataset/hw1/train/\"\n",
        "for i in classes:\n",
        "  for j in os.listdir(path+i):\n",
        "\n",
        "    file1 = open(path+i+\"/\"+j, errors=\"ignore\")\n",
        "    file=file1.read().split(\"\\n\")\n",
        "    dictionary.append(' '.join(file))\n",
        "    lbl.append(i)\n",
        "\n",
        "print(len(dictionary))\n",
        "X=generate_bow(dictionary)\n",
        "\n",
        "import pandas as pd\n",
        "y = lbl\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.25)\n",
        "\n",
        "sgd = SGDClassifier(loss='hinge', max_iter=1000, tol=0.001, shuffle=True)\n",
        "# sgd.fit(X_train, y_train)\n",
        "param_grid = {'loss':['hinge', 'log'], \n",
        "        'max_iter':[10, 100, 1000],\n",
        "        'tol':[0.01, 0.001, 0.0001],\n",
        "        'n_jobs':[-1, -2, -3],\n",
        "        'penalty':['l1', 'l2']\n",
        "        }\n",
        "\n",
        "grid = GridSearchCV(sgd, param_grid=param_grid, scoring=\"accuracy\", n_jobs=-1, cv=2)  \n",
        "grid.fit(X_train, y_train)     \n",
        "yP = grid.predict(X_test)\n",
        "accuracy = len([y_test[i] for i in range(0, len(y_test)) if y_test[i] == yP[i]]) / len(y_test)\n",
        "print(\"BOW Accuracy\", accuracy)\n",
        "print(\"Best Grid Params\", grid.best_params_)\n",
        "print(\"Best Grid Score\", grid.best_score_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "463\n",
            "10028\n",
            "BOW Accuracy 0.8879310344827587\n",
            "Best Grid Params {'loss': 'hinge', 'max_iter': 10, 'n_jobs': -2, 'penalty': 'l1', 'tol': 0.0001}\n",
            "Best Grid Score 0.9337585542488871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2lBVJHp8Npr"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFY0Fm1eLmbD",
        "outputId": "609c1db8-5211-4553-e59d-882f13391efc"
      },
      "source": [
        "X=generate_bernoulli(dictionary)\n",
        "\n",
        "y = lbl\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.25)\n",
        "sgd = SGDClassifier(loss='hinge', max_iter=1000, tol=0.001, shuffle=True)\n",
        "grid = GridSearchCV(sgd, param_grid=param_grid, scoring=\"accuracy\", n_jobs=-1, cv=2)  \n",
        "grid.fit(X_train, y_train)     \n",
        "yP = grid.predict(X_test)\n",
        "accuracy = len([y_test[i] for i in range(0, len(y_test)) if y_test[i] == yP[i]]) / len(y_test)\n",
        "print(\"Bernoulli Accuracy\", accuracy)\n",
        "print(\"Best Grid Params\", grid.best_params_)\n",
        "print(\"Best Grid Score\", grid.best_score_)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10028\n",
            "Bernoulli Accuracy 0.9396551724137931\n",
            "Best Grid Params {'loss': 'log', 'max_iter': 100, 'n_jobs': -2, 'penalty': 'l2', 'tol': 0.0001}\n",
            "Best Grid Score 0.9452528071224503\n"
          ]
        }
      ]
    }
  ]
}